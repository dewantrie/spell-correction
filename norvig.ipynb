{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dony.dewantrie/anaconda3/envs/py39/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())  \n",
    "words = Counter(words(open('./data/dict.txt').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'toyota': 1, 'avanza': 1, 'innova': 1, 'kijang': 1, 'agya': 1, 'calya': 1, 'fortuner': 1, 'yaris': 1, 'honda': 1, 'brio': 1, 'hrv': 1, 'brv': 1, 'crv': 1, 'crz': 1, 'mobilio': 1, 'jazz': 1, 'daihatsu': 1, 'go': 1, 'panca': 1, 'ayla': 1})\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellCorrector:\n",
    "    \"\"\"\n",
    "    The SpellCorrector extends the functionality of the Peter Norvig's\n",
    "    spell-corrector in http://norvig.com/spell-correct.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        :param corpus: the statistics from which corpus to use for the spell correction.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.WORDS = words\n",
    "\n",
    "    @staticmethod\n",
    "    def edit_step(word):\n",
    "        \"\"\"\n",
    "        All edits that are one edit away from `word`.\n",
    "        \"\"\"\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(self, word):\n",
    "        \"\"\"\n",
    "        All edits that are two edits away from `word`.\n",
    "        \"\"\"\n",
    "        return (e2 for e1 in self.edit_step(word)\n",
    "                for e2 in self.edit_step(e1))\n",
    "\n",
    "    def known(self, words):\n",
    "        \"\"\"\n",
    "        The subset of `words` that appear in the dictionary of WORDS.\n",
    "        \"\"\"\n",
    "        return set(w for w in words if w in self.WORDS)\n",
    "\n",
    "    def candidates(self, word):\n",
    "        ttt = self.known(self.edit_step(word)) or self.known(self.edits2(word)) or {word}\n",
    "        ttt = self.known([word]) | ttt\n",
    "        return list(ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toyota **mask**'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'toyota apanja'\n",
    "text_mask = text.replace('apanja', '**mask**')\n",
    "text_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toyota panca', 'toyota avanza']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrector = SpellCorrector()\n",
    "possible_states = corrector.candidates('apanja')\n",
    "replaced_masks = [text_mask.replace('**mask**', state) for state in possible_states]\n",
    "replaced_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertModel.\n",
      "\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "model = TFAutoModel.from_pretrained(\"indobenchmark/indobert-base-p2\", from_pt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['toyota', 'panca'], ['toyota', 'avanza'])\n",
      "([[2, 4, 15872, 3], [2, 4966, 4, 3]], [[2, 4, 11361, 3], [2, 4966, 4, 3]])\n",
      "([4966, 15872], [4966, 11361])\n"
     ]
    }
   ],
   "source": [
    "def get_ids(mask):\n",
    "    tokens = tokenizer.tokenize(mask)\n",
    "\n",
    "    input_ids =[]\n",
    "    for i in range(len(tokens)):\n",
    "        masked_tokens = tokens[:]\n",
    "        masked_tokens[i] = \"[MASK]\"\n",
    "        masked_tokens = [\"[CLS]\"] + masked_tokens + [\"[SEP]\"]\n",
    "        masked_ids = tokenizer.convert_tokens_to_ids(masked_tokens)\n",
    "        input_ids.append(masked_ids)\n",
    "\n",
    "\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return tokens, input_ids, tokens_ids\n",
    "\n",
    "ids = [get_ids(mask) for mask in replaced_masks]\n",
    "tokens, input_ids, tokens_ids = list(zip(*ids))\n",
    "\n",
    "print(tokens)\n",
    "print(input_ids)\n",
    "print(tokens_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 4, 15872, 3], [2, 4966, 4, 3], [2, 4, 11361, 3], [2, 4966, 4, 3]]\n",
      "[0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "indices, ids = [], []\n",
    "for i in range(len(input_ids)):\n",
    "    indices.extend([i] * len(input_ids[i]))\n",
    "    ids.extend(input_ids[i])\n",
    "\n",
    "indices = np.array(indices) \n",
    "    \n",
    "print(ids)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2     4 15872     3]\n",
      " [    2  4966     4     3]\n",
      " [    2     4 11361     3]\n",
      " [    2  4966     4     3]]\n"
     ]
    }
   ],
   "source": [
    "masked_padded = tf.keras.preprocessing.sequence.pad_sequences(ids, padding='post')\n",
    "print(masked_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Argument `axis` = 2 not in range [-2, 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m batch_size \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mshape(logits)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      6\u001b[0m seq_length \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mshape(logits)[\u001b[39m1\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m gather_indices \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mstack([tf\u001b[39m.\u001b[39;49mrange(batch_size, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mint32),\n\u001b[1;32m      8\u001b[0m                            tf\u001b[39m.\u001b[39;49mrange(seq_length, dtype\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mint32),\n\u001b[1;32m      9\u001b[0m                            logits], axis\u001b[39m=\u001b[39;49m\u001b[39m2\u001b[39;49m)\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/envs/py39/lib/python3.9/site-packages/tensorflow/python/ops/array_ops_stack.py:82\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(values, axis, name)\u001b[0m\n\u001b[1;32m     80\u001b[0m   expanded_num_dims \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(value_shape) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     81\u001b[0m   \u001b[39mif\u001b[39;00m axis \u001b[39m<\u001b[39m \u001b[39m-\u001b[39mexpanded_num_dims \u001b[39mor\u001b[39;00m axis \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m expanded_num_dims:\n\u001b[0;32m---> 82\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mArgument `axis` = \u001b[39m\u001b[39m{\u001b[39;00maxis\u001b[39m}\u001b[39;00m\u001b[39m not in range \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39m-\u001b[39mexpanded_num_dims\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{\u001b[39;00mexpanded_num_dims\u001b[39m}\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[39mreturn\u001b[39;00m gen_array_ops\u001b[39m.\u001b[39mpack(values, axis\u001b[39m=\u001b[39maxis, name\u001b[39m=\u001b[39mname)\n",
      "\u001b[0;31mValueError\u001b[0m: Argument `axis` = 2 not in range [-2, 2)"
     ]
    }
   ],
   "source": [
    "outputs = model(masked_padded)\n",
    "logits = outputs.last_hidden_state\n",
    "preds = tf.nn.softmax(logits, axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.999999761581421, 7.999999701976776]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    filter_preds = preds[indices == i]\n",
    "    total = tf.reduce_sum(filter_preds, axis=2).numpy().flatten()\n",
    "\n",
    "    sum = 0\n",
    "    for k in range(len(total)):\n",
    "        sum += total[k]\n",
    "    \n",
    "    scores.append(sum)\n",
    "    \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('avanza', 0.4999999981373548), ('panca', 0.5000000018626453)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_scores = np.array(scores) / np.sum(scores)\n",
    "probs = list(zip(possible_states, prob_scores))\n",
    "probs.sort(key = lambda x: x[1])  \n",
    "probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
