{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())  \n",
    "words = Counter(words(open('./data/dict.txt').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'toyota': 1, 'avanza': 1, 'innova': 1, 'kijang': 1, 'agya': 1, 'calya': 1, 'fortuner': 1, 'yaris': 1, 'honda': 1, 'brio': 1, 'hrv': 1, 'brv': 1, 'crv': 1, 'crz': 1, 'mobilio': 1, 'jazz': 1, 'daihatsu': 1, 'go': 1, 'panca': 1, 'ayla': 1})\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpellCorrector:\n",
    "    \"\"\"\n",
    "    The SpellCorrector extends the functionality of the Peter Norvig's\n",
    "    spell-corrector in http://norvig.com/spell-correct.html\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        :param corpus: the statistics from which corpus to use for the spell correction.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.WORDS = words\n",
    "\n",
    "    @staticmethod\n",
    "    def edit_step(word):\n",
    "        \"\"\"\n",
    "        All edits that are one edit away from `word`.\n",
    "        \"\"\"\n",
    "        letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "        deletes = [L + R[1:] for L, R in splits if R]\n",
    "        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "        replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "        inserts = [L + c + R for L, R in splits for c in letters]\n",
    "        return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "    def edits2(self, word):\n",
    "        \"\"\"\n",
    "        All edits that are two edits away from `word`.\n",
    "        \"\"\"\n",
    "        return (e2 for e1 in self.edit_step(word)\n",
    "                for e2 in self.edit_step(e1))\n",
    "\n",
    "    def known(self, words):\n",
    "        \"\"\"\n",
    "        The subset of `words` that appear in the dictionary of WORDS.\n",
    "        \"\"\"\n",
    "        return set(w for w in words if w in self.WORDS)\n",
    "\n",
    "    def candidates(self, word):\n",
    "        ttt = self.known(self.edit_step(word)) or self.known(self.edits2(word)) or {word}\n",
    "        ttt = self.known([word]) | ttt\n",
    "        return list(ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toyota **mask**'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'toyota apanja'\n",
    "text_mask = text.replace('apanja', '**mask**')\n",
    "text_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['toyota panca', 'toyota avanza']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrector = SpellCorrector()\n",
    "possible_states = corrector.candidates('apanja')\n",
    "replaced_masks = [text_mask.replace('**mask**', state) for state in possible_states]\n",
    "replaced_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertModel.\n",
      "\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p2\")\n",
    "model = TFAutoModel.from_pretrained(\"indobenchmark/indobert-base-p2\", from_pt=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['toyota', 'panca'], ['toyota', 'avanza'])\n",
      "([[2, 4, 15872, 3], [2, 4966, 4, 3]], [[2, 4, 11361, 3], [2, 4966, 4, 3]])\n",
      "([4966, 15872], [4966, 11361])\n"
     ]
    }
   ],
   "source": [
    "def get_ids(mask):\n",
    "    tokens = tokenizer.tokenize(mask)\n",
    "\n",
    "    input_ids =[]\n",
    "    for i in range(len(tokens)):\n",
    "        masked_tokens = tokens[:]\n",
    "        masked_tokens[i] = \"[MASK]\"\n",
    "        masked_tokens = [\"[CLS]\"] + masked_tokens + [\"[SEP]\"]\n",
    "        masked_ids = tokenizer.convert_tokens_to_ids(masked_tokens)\n",
    "        input_ids.append(masked_ids)\n",
    "\n",
    "\n",
    "    tokens_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "    return tokens, input_ids, tokens_ids\n",
    "\n",
    "ids = [get_ids(mask) for mask in replaced_masks]\n",
    "tokens, input_ids, tokens_ids = list(zip(*ids))\n",
    "\n",
    "print(tokens)\n",
    "print(input_ids)\n",
    "print(tokens_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 4, 15872, 3], [2, 4966, 4, 3], [2, 4, 11361, 3], [2, 4966, 4, 3]]\n",
      "[0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "indices, ids = [], []\n",
    "for i in range(len(input_ids)):\n",
    "    indices.extend([i] * len(input_ids[i]))\n",
    "    ids.extend(input_ids[i])\n",
    "\n",
    "indices = np.array(indices) \n",
    "    \n",
    "print(ids)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    2     4 15872     3]\n",
      " [    2  4966     4     3]\n",
      " [    2     4 11361     3]\n",
      " [    2  4966     4     3]]\n"
     ]
    }
   ],
   "source": [
    "masked_padded = tf.keras.preprocessing.sequence.pad_sequences(ids, padding='post')\n",
    "print(masked_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 4, 768), dtype=float32, numpy=\n",
       "array([[[0.0006597 , 0.00558194, 0.00150794, ..., 0.00678939,\n",
       "         0.00180527, 0.00081173],\n",
       "        [0.00037884, 0.01029356, 0.00229337, ..., 0.00395984,\n",
       "         0.00139592, 0.00025384],\n",
       "        [0.00029957, 0.00326895, 0.00276056, ..., 0.01808298,\n",
       "         0.0023506 , 0.0005481 ],\n",
       "        [0.0002503 , 0.00220146, 0.00176794, ..., 0.01033895,\n",
       "         0.0012267 , 0.0006596 ]],\n",
       "\n",
       "       [[0.00044055, 0.00271862, 0.00095102, ..., 0.00829295,\n",
       "         0.00137999, 0.0007511 ],\n",
       "        [0.00048425, 0.00154953, 0.00247285, ..., 0.00223989,\n",
       "         0.00129973, 0.00111753],\n",
       "        [0.00037039, 0.00162738, 0.00116848, ..., 0.01852681,\n",
       "         0.00041396, 0.0003057 ],\n",
       "        [0.0002731 , 0.00093676, 0.0014088 , ..., 0.0054424 ,\n",
       "         0.00079327, 0.0007274 ]],\n",
       "\n",
       "       [[0.00172364, 0.00333404, 0.00107855, ..., 0.004449  ,\n",
       "         0.00109876, 0.00140676],\n",
       "        [0.00202484, 0.00192249, 0.00216847, ..., 0.00106051,\n",
       "         0.00117942, 0.00127537],\n",
       "        [0.00162626, 0.0013683 , 0.00094418, ..., 0.00536047,\n",
       "         0.00145386, 0.00106313],\n",
       "        [0.00089439, 0.0014548 , 0.00171672, ..., 0.00555363,\n",
       "         0.00073503, 0.00137816]],\n",
       "\n",
       "       [[0.00044055, 0.00271862, 0.00095102, ..., 0.00829295,\n",
       "         0.00137999, 0.0007511 ],\n",
       "        [0.00048425, 0.00154953, 0.00247285, ..., 0.00223989,\n",
       "         0.00129973, 0.00111753],\n",
       "        [0.00037039, 0.00162738, 0.00116848, ..., 0.01852681,\n",
       "         0.00041396, 0.0003057 ],\n",
       "        [0.0002731 , 0.00093676, 0.0014088 , ..., 0.0054424 ,\n",
       "         0.00079327, 0.0007274 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs = model(masked_padded)\n",
    "logits = outputs.last_hidden_state\n",
    "preds = tf.nn.softmax(logits, axis=2)\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.999999761581421, 7.999999701976776]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "scores = []\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    filter_preds = preds[indices == i]\n",
    "    total = tf.reduce_sum(filter_preds, axis=2).numpy().flatten()\n",
    "\n",
    "    sum = 0\n",
    "    for k in range(len(total)):\n",
    "        sum += total[k]\n",
    "    \n",
    "    scores.append(sum)\n",
    "    \n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('avanza', 0.4999999981373548), ('panca', 0.5000000018626453)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob_scores = np.array(scores) / np.sum(scores)\n",
    "probs = list(zip(possible_states, prob_scores))\n",
    "probs.sort(key = lambda x: x[1])  \n",
    "probs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
